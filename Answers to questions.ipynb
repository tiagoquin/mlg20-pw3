{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore the “hold_out_validation” notebook\n",
    "\n",
    "### Q1. Determine where do we define all the above mentioned parameters. Observe that we run the evaluation procedure on four different problems. Each problem is a two-class two-dimensional problem, where the two sets are more and more overlapped (e.g., the synthetic datasets are randomly generated using variances of 0.4, 0.5, 0.6 and 0.7).\n",
    "\n",
    "```python\n",
    "N_INITS = 2\n",
    "N_SPLITS = 10\n",
    "DATASET_SIZE = 200\n",
    "EPOCHS = 100\n",
    "N_NEURONS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "MOMENTUM = 0.7\n",
    "TRAIN_TEST_RATIO = 0.8\n",
    "DATA_PARAMS = np.arange(0.4, 0.71, 0.1)\n",
    "```\n",
    "\n",
    "### Q2. What are the cyan and red curves in those plots ? Why are they different ?\n",
    "\n",
    " - Red: Mean Square Error function for the different hold out runs test\n",
    " - Cyan : Same for training\n",
    "\n",
    "\n",
    "### Q3. What happens with the training and test errors (MSE) when we have the two sets more overlapped ?\n",
    "\n",
    "When they are more overlapped, we have more coherent results so we can say that our model is better.\n",
    "\n",
    "\n",
    "### Q4. Why sometimes the red curves indicate a higher error than the cyan ones ?\n",
    "\n",
    "Since we cut our dataset randomly, we might have trained our network with more specific data. So it doesn't account  for the features well.\n",
    "\n",
    "### Q5. What is showing the boxplot summarizing the validation errors of the preceding experiments ?\n",
    "\n",
    "With the lower spread of 0.4, the MSE are way more overlaped than in others. \n",
    "As we have more difficult problems (higher spread), the less coherent become the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the “cross_validation” notebook\n",
    "\n",
    "### Q1. Determine where do we define all the above mentioned parameters.\n",
    "\n",
    "```python\n",
    "N_SPLITS = 10\n",
    "DATASET_SIZE = 200\n",
    "EPOCHS = 20\n",
    "N_NEURONS = 2\n",
    "K = 5\n",
    "LEARNING_RATE = 0.001\n",
    "MOMENTUM = 0.7\n",
    "DATA_PARAMS = np.arange(0.4, 0.71, 0.1)\n",
    "```\n",
    "\n",
    "### Q2. What is the difference between hold-out and cross-validation ? What is the new parameter that has to be defined for cross-validation ?\n",
    "\n",
    "The new parameter is K. Here we split the validation in 5 chunks. \n",
    "We repeat 5 times the experience. Where each time a different chunk is used as a test set, and the 4 others are used as training sets.\n",
    "\n",
    "It would look like:\n",
    "\n",
    " - train: AB, test C\n",
    " - train: AC, test B\n",
    " - train: BC, test A\n",
    "\n",
    "### Q3. Observe the boxplots summarizing the validation errors obtained using the crossvalidation method and compare them with those obtained by hold-out validation. \n",
    "\n",
    "The boxplots present results of the MSE functions that are way more overlaped. Which is more coherent and thus better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
